{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Members\n",
    "- Cao Thanh Khiết   19120544\n",
    "- Ninh Duy Huy      19120533\n",
    "- Lê Minh Hữu       19120525\n",
    "- Nguyễn Tuấn Khoa  19120547\n",
    "- Trần Tuấn Kiệt    19120557"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from vncorenlp import VnCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vncorenlp_file = \"VnCoreNLP/VnCoreNLP-1.1.1.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thủ tướng Abe cúi đầu xin lỗi vì hành động phi...</td>\n",
       "      <td>binhluan.biz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thủ tướng Nhật cúi đầu xin lỗi vì tinh thần ph...</td>\n",
       "      <td>www.ipick.vn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Choáng! Cơ trưởng đeo khăn quàng quẩy banh nóc...</td>\n",
       "      <td>tintucqpvn.net</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chưa bao giờ nhạc Kpop lại dễ hát đến thế!!!\\n...</td>\n",
       "      <td>tintucqpvn.net</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Đại học Hutech sẽ áp dụng cải cách \"Tiếq Việt\"...</td>\n",
       "      <td>www.gioitreviet.net</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text               domain  \\\n",
       "0  Thủ tướng Abe cúi đầu xin lỗi vì hành động phi...         binhluan.biz   \n",
       "1  Thủ tướng Nhật cúi đầu xin lỗi vì tinh thần ph...         www.ipick.vn   \n",
       "2  Choáng! Cơ trưởng đeo khăn quàng quẩy banh nóc...       tintucqpvn.net   \n",
       "3  Chưa bao giờ nhạc Kpop lại dễ hát đến thế!!!\\n...       tintucqpvn.net   \n",
       "4  Đại học Hutech sẽ áp dụng cải cách \"Tiếq Việt\"...  www.gioitreviet.net   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"vfnd/CSV/vn_news_223_tdlfr.csv\")\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 223 entries, 0 to 222\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    223 non-null    object\n",
      " 1   domain  223 non-null    object\n",
      " 2   label   223 non-null    int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 5.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List of vietnamese stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_stopwords = []\n",
    "with open('data/vietnamese_stopwords.txt', encoding=\"utf8\") as file:\n",
    "    for line in file.read().splitlines():\n",
    "        vn_stopwords.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA (Exploratory Data analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dữ liệu có bị lặp không"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.index.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dữ liệu có null hay không"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      0\n",
       "domain    0\n",
       "label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sự phân bố các lớp label có lệch nhau không"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    55.156951\n",
       "1    44.843049\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df.label.value_counts(normalize=True).plot(kind=\"bar\", color=[\"red\", \"blue\"])\n",
    "data_df['label'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Sự phân bố giữa 2 lớp khá tương đương nhau, không lệch nhau nhiều"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chiều dài trung bình các record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2539.77"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_record = []\n",
    "text_series = data_df['text']\n",
    "for text in text_series:\n",
    "    len_record.append(len(text))\n",
    "len_record = pd.Series(len_record)\n",
    "round(len_record.sum() / len(len_record), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We will extract X_train and Y_train, X_train will contains only column \"text\", Y_train contains column \"label\"\n",
    "'''\n",
    "X_df = data_df.iloc[:, :-2].values\n",
    "Y_target = data_df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loại bỏ một số giá trị không cần thiết\n",
    "- Loại bỏ stopwords\n",
    "- Tokenize đoạn văn thành những từ có nghĩa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X_df):\n",
    "\n",
    "    for index, test_str in enumerate(X_df):\n",
    "        # Remove http links\n",
    "        filtered_str = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", test_str[0])\n",
    "        # Remove all number in string\n",
    "        filtered_str = ''.join([i for i in filtered_str if not i.isdigit()])\n",
    "        # Remove all special characters and punctuation\n",
    "        filtered_str = re.sub('\\W+',' ', filtered_str)\n",
    "        filtered_str = filtered_str.strip()\n",
    "        X_df[index][0] = filtered_str.lower()\n",
    "\n",
    "    X_df = X_df.flatten()\n",
    "\n",
    "\n",
    "    # Remove stop words\n",
    "    for index, test_str in enumerate(X_df):\n",
    "        sw_removed = [w for w in test_str.split(' ') if not w in vn_stopwords]\n",
    "        X_df[index] = ' '.join(sw_removed)\n",
    "\n",
    "    return X_df\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "def tokennize_text(X_df):\n",
    "    '''\n",
    "    return a numpy array of preprocessed text, each item in array is a paragraph after remove stopwords, special characters, ...\n",
    "    '''\n",
    "    # annotator = VnCoreNLP(vncorenlp_file, annotators=\"wseg,pos,ner,parse\", max_heap_size='-Xmx2g')\n",
    "    annotator = VnCoreNLP(vncorenlp_file)\n",
    "    with annotator:\n",
    "        for index, filtered_text in enumerate(X_df):\n",
    "            token_text = annotator.tokenize(filtered_text)[0]\n",
    "            X_df[index] = ' '.join(token_text)\n",
    "    \n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now, apply preprocessing into my text\n",
    "'''\n",
    "X_df = preprocessing(X_df)\n",
    "X_df = tokennize_text(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Extracting feature from text data\n",
    "We use Bag of words model for converting text to numerical data\n",
    "Now, we create a Document Term Matrix (Bag of Words), we will vectorize the normal text\n",
    "'''\n",
    "# Use CountVectorizer to convert text data into numerical values\n",
    "def create_DTM(X_df):\n",
    "    cv = CountVectorizer(analyzer='word')\n",
    "    data = cv.fit_transform(X_df).todense()\n",
    "    return data\n",
    "\n",
    "\n",
    "'''TF-IDF'''\n",
    "def vectorize_tfidf(X_df):\n",
    "    vectorization = TfidfVectorizer()\n",
    "    return vectorization.fit_transform(X_df).todense()\n",
    "\n",
    "# vectorization = TfidfVectorizer()\n",
    "# xv_train = vectorization.fit_transform(X_df).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,

   "metadata": {},
   "outputs": [],
   "source": [
    "'''Split data into train-test set'''\n",

    "X_train_vect, X_test_vect, y_train, y_test = train_test_split(X_df, Y_target, test_size=0.2, random_state=0)\n",
    "\n",
    "'''Fit and transform model'''\n",

    "vectorization = TfidfVectorizer()\n",
    "X_train = vectorization.fit_transform(X_train_vect)\n",
    "X_test = vectorization.transform(X_test_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 15,

   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 86.67%\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "acc_test = round(LR.score(X_test, y_test)*100, 2)\n",
    "print(f\"Accuracy on test set: {acc_test}%\")"
   ]
  },
  {
   "cell_type": "code",
    
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0], dtype=int64)"
      ]
     },

     "execution_count": 17,

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_LR = LR.predict(X_test)\n",
    "pred_LR"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25,  2],\n",
       "       [ 4, 14]], dtype=int64)"
      ]
     },

     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfs_mat = confusion_matrix(y_test, pred_LR)\n",
    "cfs_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [

       "array([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",

       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0], dtype=int64)"
      ]
     },

     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Build model and predict in test dataset\n",
    "'''\n",
    "dtc = DecisionTreeClassifier(criterion='entropy')\n",
    "dtc.fit(X_train, y_train)\n",
    "y_pred = dtc.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23,  4],\n",

       "       [ 7, 11]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfs_mat = confusion_matrix(y_test, y_pred)\n",
    "cfs_mat"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 77.78%\n"
     ]
    }
   ],
   "source": [
    "# accuracy = round((cfs_mat[0][0] + cfs_mat[1][1]) / (cfs_mat[0][0] + cfs_mat[0][1] + cfs_mat[1][0] + cfs_mat[1][1])*100, 2)\n",
    "acc_test = round(dtc.score(X_test, y_test)*100, 2)\n",
    "print(f\"Accuracy on test set: {acc_test}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Booting Classifier"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 75.56%\n"
     ]
    }
   ],
   "source": [
    "GBC = GradientBoostingClassifier(random_state=0)\n",
    "GBC.fit(X_train, y_train)\n",
    "GBC_accur = round(GBC.score(X_test, y_test)*100, 2)\n",
    "print(f\"Accuracy on test set: {GBC_accur}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0], dtype=int64)"
      ]
     },

     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_GBC = GBC.predict(X_test)\n",
    "pred_GBC"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24,  3],\n",
       "       [ 8, 10]], dtype=int64)"
      ]
     },

     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfs_mat = confusion_matrix(y_test, pred_GBC)\n",
    "cfs_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearRegession"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [

      "Accuracy on test set: 86.67%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "REG = linear_model.LinearRegression()\n",
    "REG.fit(X_train, y_train)\n",
    "y_pred = REG.predict(X_test)\n",
    "y_pred[y_pred < 0.5] = 0\n",
    "y_pred[y_pred >= 0.5] = 1\n",
    "REG_accur = round(accuracy_score(y_test, y_pred)*100, 2)\n",
    "print(f\"Accuracy on test set: {REG_accur}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Centroid Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "clf = NearestCentroid()\n",
    "clf.fit(X_train, y_train)\n",
    "NC_pred = clf.predict(X_test)\n",
    "NC_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24,  3],\n",
       "       [ 2, 16]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfs_mat = confusion_matrix(y_test, NC_pred)\n",
    "cfs_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 88.89%\n"
     ]
    }
   ],
   "source": [
    "NC_accur = round(clf.score(X_test, y_test)*100, 2)\n",
    "print(f\"Accuracy on test set: {NC_accur}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual testing"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(predict, name):\n",
    "    print(f\"{name} predicts: \", end='')\n",
    "    if predict[0] == 0:\n",
    "        print(\"Fake news\")\n",
    "    else:\n",
    "        print(\"is not Fake\")\n",
    "\n",
    "def manual_testing(news):\n",
    "# news = \"cao thanh khiết víp pro hahaaha cuộc sống này thật tươi đẹp, quang hải đã giành chức vô địch ở giải thế giới\"\n",
    "\n",
    "    test_dict = {\"text\": [news]}\n",
    "    test_df = pd.DataFrame(test_dict)\n",
    "\n",
    "    text = test_df.iloc[:,:].values\n",
    "\n",
    "    processed_text = tokennize_text(preprocessing(text))\n",
    "\n",
    "    vector_text = vectorization.transform(processed_text)\n",
    "\n",

    "    return LR.predict(vector_text), dtc.predict(vector_text), GBC.predict(vector_text), clf.predict(vector_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic_Regression  predicts: Fake news\n",
      "Decision_Tree  predicts: is not Fake\n",
      "Gradient_Booting_Classifier  predicts: Fake news\n",
      "Nearest_Centroid_Classifier  predicts: Fake news\n"
     ]
    }
   ],
   "source": [
    "news = \"Tại buổi họp báo, ông Huỳnh Thuận, phó giám đốc Sở Y tế Quảng Nam, cho biết ngành y tế không mua kit test xét nghiệm Covid-19 của Công ty Việt Á. Trước năm 2021, có mượn máy xét nghiệm của công ty này để sử dụng và đến đầu năm 2021 đã trả lại. Phát biểu kết luận, Phó chủ tịch UBND tỉnh Quảng Nam Trần Văn Tân cho biết về việc tỉnh Quảng Nam mượn máy xét nghiệm của Công ty Việt Á thì tỉnh đã chủ động cung cấp thông tin cho báo chí.Theo ông Tân, năm 2021 tỉnh không mua kit test xét nghiệm Covid-19 của Công ty Việt Á mà chỉ mượn máy xét nghiệm xong rồi trả.\"\n",
    "\n",
    "lr_pred, dtc_pred, gbc_pred, ncc_pred = manual_testing(news)\n",
    "print_result(lr_pred, \"Logistic_Regression \")\n",
    "print_result(dtc_pred, \"Decision_Tree \")\n",
    "print_result(gbc_pred, \"Gradient_Booting_Classifier \")\n",
    "print_result(ncc_pred, \"Nearest_Centroid_Classifier \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "628dd01b888ab9cdfddc3a0771c5db8a998a3af968dbb11740c2370c8ddf1b03"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
